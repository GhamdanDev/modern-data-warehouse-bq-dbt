{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“ Workshop: Building a Modern Data Warehouse with BigQuery & dbt\n",
        "\n",
        "Welcome to this advanced hands-on lab. Today, we will transform raw e-commerce data into a professional **Star Schema** Data Warehouse using the **ELT (Extract, Load, Transform)** pattern.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸš€ Learning Objectives\n",
        "By the end of this session, you will master the end-to-end Data Engineering lifecycle:\n",
        "\n",
        "1.  **â˜ï¸ Cloud Setup & Infrastructure:**\n",
        "    *   Authenticate with **Google Cloud Platform (GCP)**.\n",
        "    *   Set up a serverless development environment in Colab.\n",
        "    \n",
        "2.  **ğŸ“¥ Data Ingestion (Extract & Load):**\n",
        "    *   Automate downloading the **Brazilian E-Commerce Dataset** (100k+ orders) using the Kaggle API.\n",
        "    *   Build a **Smart Uploader** in Python to sanitize and load raw CSVs into **BigQuery** (`ecommerce_raw`).\n",
        "\n",
        "3.  **ğŸ”§ Code Refactoring & Automation:**\n",
        "    *   Learn how to programmatically refactor a legacy **dbt (Data Build Tool)** project.\n",
        "    *   Use Python scripting to update Project IDs and fix SQL syntax errors across dozens of files instantly.\n",
        "\n",
        "4.  **ğŸ—ï¸ Analytics Engineering (Transform):**\n",
        "    *   Execute a **dbt pipeline** to create:\n",
        "        *   **ğŸ¥ˆ Staging Layer:** Cleaned and standardized views/tables.\n",
        "        *   **ğŸ¥‡ Marts Layer:** Business-ready Fact and Dimension tables (Star Schema).\n",
        "\n",
        "---\n",
        "### ğŸ› ï¸ Tech Stack\n",
        "*   **Language:** Python 3.10+\n",
        "*   **Warehouse:** Google BigQuery (Standard SQL)\n",
        "*   **Transformation:** dbt (Data Build Tool) Core 1.x\n",
        "*   **Source:** Kaggle API\n",
        "\n",
        "---\n",
        "### âš ï¸ Before We Begin\n",
        "*   Ensure you have a valid **Google Cloud Project ID** ready.\n",
        "*   The code below uses automated scripts to fix common configuration errorsâ€”run the cells sequentially.\n",
        "\n",
        "*Let's build a production-grade pipeline!* ğŸ‘‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BVoH-geebD_",
        "outputId": "0836334b-75ae-451c-f369-dfc904697d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.4/114.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.9/144.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.7/442.7 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.8/162.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.4/91.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCloning into 'modern-data-warehouse-bq-dbt'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 31 (delta 8), reused 31 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (31/31), 5.94 KiB | 2.97 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "/content/modern-data-warehouse-bq-dbt/GCP-COLAB-DBT/pipeline-ecommerce-bq-dbt/cloud_run_dbt/dbt\n",
            "âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ù‚Ø© Ø¨Ù†Ø¬Ø§Ø­!\n"
          ]
        }
      ],
      "source": [
        "# 1. Google Cloud Authentication\n",
        "# Authenticate the user with Google Cloud Platform to allow access to BigQuery and other GCP services.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# 2. Install Required Tools\n",
        "# Install necessary Python libraries including dbt-bigquery adapter, opendatasets, and pandas-gbq for data manipulation and BigQuery interaction.\n",
        "!pip install -q dbt-bigquery opendatasets pandas-gbq\n",
        "\n",
        "# 3. Project Configuration\n",
        "# Set the Google Cloud Project ID. Ensure this matches your specific GCP project identifier where BigQuery resources will be provisioned.\n",
        "MY_PROJECT_ID = \"astute-backup-394812\"\n",
        "\n",
        "# 4. Clone Repository\n",
        "# Clone the project repository from GitHub if the directory does not already exist. This pulls the latest dbt project structure.\n",
        "import os\n",
        "if not os.path.exists('data_engineering_projects'):\n",
        "    !git clone https://github.com/GhamdanDev/modern-data-warehouse-bq-dbt.git\n",
        "\n",
        "# 5. Navigate to Project Directory\n",
        "# Change the current working directory to the dbt project folder to execute dbt commands.\n",
        "%cd /content/modern-data-warehouse-bq-dbt/GCP-COLAB-DBT/pipeline-ecommerce-bq-dbt/cloud_run_dbt/dbt\n",
        "\n",
        "print(\"âœ… Environment setup and authentication completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPD8qWg4fy9k",
        "outputId": "60cc61d0-cd94-46ca-ea96-d2291de7c732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Kaggle...\n",
            "Dataset URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading brazilian-ecommerce.zip to /content/brazilian-ecommerce\n",
            "  0% 0.00/42.6M [00:00<?, ?B/s]\n",
            "100% 42.6M/42.6M [00:00<00:00, 1.46GB/s]\n",
            "âœ¨ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Dataset Ø¬Ø¯ÙŠØ¯ Ø¨Ø§Ø³Ù… 'ecommerce_raw'.\n",
            "\n",
            "ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø±ÙØ¹ Ø¥Ù„Ù‰ BigQuery...\n",
            "â³ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ olist_customers_dataset (99441 Ø³Ø¬Ù„)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2668302334.py:64: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  df.to_gbq(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ù†Ø¬Ø­ Ø±ÙØ¹ Ø§Ù„Ø¬Ø¯ÙˆÙ„: olist_customers_dataset\n",
            "â³ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ olist_orders_dataset (99441 Ø³Ø¬Ù„)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2668302334.py:64: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  df.to_gbq(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ù†Ø¬Ø­ Ø±ÙØ¹ Ø§Ù„Ø¬Ø¯ÙˆÙ„: olist_orders_dataset\n",
            "â³ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ olist_products_dataset (32951 Ø³Ø¬Ù„)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2668302334.py:64: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  df.to_gbq(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ù†Ø¬Ø­ Ø±ÙØ¹ Ø§Ù„Ø¬Ø¯ÙˆÙ„: olist_products_dataset\n",
            "â³ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ olist_sellers_dataset (3095 Ø³Ø¬Ù„)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2668302334.py:64: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  df.to_gbq(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ù†Ø¬Ø­ Ø±ÙØ¹ Ø§Ù„Ø¬Ø¯ÙˆÙ„: olist_sellers_dataset\n",
            "â³ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ olist_order_items_dataset (112650 Ø³Ø¬Ù„)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2668302334.py:64: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  df.to_gbq(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ù†Ø¬Ø­ Ø±ÙØ¹ Ø§Ù„Ø¬Ø¯ÙˆÙ„: olist_order_items_dataset\n",
            "â³ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ olist_order_payments_dataset (103886 Ø³Ø¬Ù„)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2668302334.py:64: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  df.to_gbq(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ù†Ø¬Ø­ Ø±ÙØ¹ Ø§Ù„Ø¬Ø¯ÙˆÙ„: olist_order_payments_dataset\n",
            "â³ Ø¬Ø§Ø±ÙŠ Ø±ÙØ¹ olist_geolocation_dataset (1000163 Ø³Ø¬Ù„)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2668302334.py:64: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
            "  df.to_gbq(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ù†Ø¬Ø­ Ø±ÙØ¹ Ø§Ù„Ø¬Ø¯ÙˆÙ„: olist_geolocation_dataset\n",
            "\n",
            "ğŸ¯ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ù…Ù‡Ù…Ø©! ÙƒØ§ÙØ© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø®Ø§Ù… Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù† Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… dbt.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# 1. Kaggle Authentication Setup\n",
        "# Programmatically configure Kaggle credentials to automate dataset downloading. \n",
        "# This prevents manual username/key prompts during execution by creating the .kaggle directory \n",
        "# and writing the API token securely.\n",
        "kaggle_config = {\"username\":\"ghamdannashwan\",\"key\":\"KGAT_148e98e849742fbd54519ad67a04d107\"}\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(kaggle_config, f)\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# 2. Download Dataset\n",
        "# Download the Brazilian E-Commerce dataset directly using the Kaggle CLI for speed and reliability.\n",
        "# The --unzip flag automatically extracts content to the specified path.\n",
        "print(\"â³ Downloading data from Kaggle...\")\n",
        "!kaggle datasets download -d olistbr/brazilian-ecommerce --unzip -p /content/brazilian-ecommerce\n",
        "\n",
        "# 3. BigQuery Configuration\n",
        "# Define project constants including the Project ID, BigQuery Dataset ID (Data Warehouse Schema), \n",
        "# and the local path where raw data is stored.\n",
        "MY_PROJECT_ID = \"astute-backup-394812\"\n",
        "DATASET_ID = \"ecommerce_raw\"\n",
        "DATA_PATH = '/content/brazilian-ecommerce'\n",
        "\n",
        "# File-to-Table Mapping\n",
        "# Define a mapping dictionary to link raw CSV filenames to their corresponding target table names in BigQuery.\n",
        "# This ensures standardized naming for the raw layer tables.\n",
        "mapping_standard = {\n",
        "    'olist_customers_dataset.csv': 'olist_customers_dataset',\n",
        "    'olist_orders_dataset.csv': 'olist_orders_dataset',\n",
        "    'olist_products_dataset.csv': 'olist_products_dataset',\n",
        "    'olist_sellers_dataset.csv': 'olist_sellers_dataset',\n",
        "    'olist_order_items_dataset.csv': 'olist_order_items_dataset',\n",
        "    'olist_order_payments_dataset.csv': 'olist_order_payments_dataset',\n",
        "    'olist_geolocation_dataset.csv': 'olist_geolocation_dataset'\n",
        "}\n",
        "\n",
        "client = bigquery.Client(project=MY_PROJECT_ID)\n",
        "\n",
        "# 4. Dataset Validation\n",
        "# Check if the target BigQuery dataset exists. If not, create it in the 'US' location \n",
        "# to ensure subsequent data loads succeed.\n",
        "dataset_ref = client.dataset(DATASET_ID)\n",
        "try:\n",
        "    client.get_dataset(dataset_ref)\n",
        "    print(f\"âœ… Dataset '{DATASET_ID}' already exists.\")\n",
        "except:\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"US\"\n",
        "    client.create_dataset(dataset)\n",
        "    print(f\"âœ¨ Created new Dataset named '{DATASET_ID}'.\")\n",
        "\n",
        "# 5. Smart Data Ingestion\n",
        "# Iterate through the file mapping to ingest CSVs into BigQuery using pandas-gbq.\n",
        "# This process includes reading, cleaning, and uploading the data.\n",
        "print(\"\\nğŸš€ Starting upload process to BigQuery...\")\n",
        "\n",
        "for file_name, table_name in mapping_standard.items():\n",
        "    full_path = os.path.join(DATA_PATH, file_name)\n",
        "\n",
        "    if os.path.exists(full_path):\n",
        "        try:\n",
        "            # Read CSV with memory optimization (low_memory=False) to handle mixed types efficiently.\n",
        "            df = pd.read_csv(full_path, low_memory=False)\n",
        "\n",
        "            # Sanitize column names by replacing spaces and periods with underscores \n",
        "            # to comply with BigQuery naming conventions.\n",
        "            df.columns = [c.replace(' ', '_').replace('.', '_') for c in df.columns]\n",
        "\n",
        "            print(f\"â³ Uploading {table_name} ({len(df)} records)...\")\n",
        "\n",
        "            # Upload DataFrame to BigQuery using the `to_gbq` method.\n",
        "            # 'if_exists=replace' overwrites the table if it already exists.\n",
        "            df.to_gbq(\n",
        "                destination_table=f\"{DATASET_ID}.{table_name}\",\n",
        "                project_id=MY_PROJECT_ID,\n",
        "                if_exists='replace',\n",
        "                progress_bar=False,\n",
        "                location='US'\n",
        "            )\n",
        "            print(f\"âœ… Successfully uploaded table: {table_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to upload {table_name}. Error: {e}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: File {file_name} not found in the specified path.\")\n",
        "\n",
        "print(\"\\nğŸ¯ Task completed! All raw tables are now ready for transformation using dbt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlcwf3bFjXOj",
        "outputId": "bc5281c5-f8cb-4be5-a596-ef207396542a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ·Ù‡ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: Ù…Ù† ecommerce-analysis-455200 Ø¥Ù„Ù‰ astute-backup-394812\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: profiles.yml\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: dim_orders.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: dim_sellers.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: fact_order_items.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: dim_products.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: fact_order_payments.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: dim_customers.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: dim_products_stg.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: dim_sellers_stg.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: fact_order_items_stg.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: fact_order_payments_stg.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: dim_customers_stg.sql\n",
            "âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ÙˆØªØ·Ù‡ÙŠØ±: dim_orders_stg.sql\n",
            "ğŸ¯ ØªÙ… Ø¥ØµÙ„Ø§Ø­ Ù…Ù„Ù dbt_project.yml Ø¨Ø§Ù„ÙƒØ§Ù…Ù„.\n",
            "\n",
            "âœ¨ Ø§Ù„Ø¢Ù† dbt Ù†Ø¸ÙŠÙ ØªÙ…Ø§Ù…Ø§Ù‹ ÙˆÙ…Ø±ØªØ¨Ø· Ø¨Ù…Ø´Ø±ÙˆØ¹Ùƒ ÙÙ‚Ø·. Ø¬Ø§Ù‡Ø² Ù„Ù„ØªÙ†ÙÙŠØ°!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# 1. Refactoring Configuration\n",
        "# Set up paths and project IDs to update the cloned dbt project with your specific GCP configuration.\n",
        "# This ensures that all dbt models point to the correct Google Cloud Project.\n",
        "DBT_PATH = \"/content/modern-data-warehouse-bq-dbt/GCP-COLAB-DBT/pipeline-ecommerce-bq-dbt/cloud_run_dbt/dbt\"\n",
        "OLD_ID = \"ecommerce-analysis-455200\"\n",
        "NEW_ID = MY_PROJECT_ID # astute-backup-394812\n",
        "\n",
        "print(f\"ğŸš€ Starting comprehensive refactoring: from {OLD_ID} to {NEW_ID}\")\n",
        "\n",
        "# 2. Comprehensive Project Refactoring Function\n",
        "# Define a robust function to walk through the project directory and replace hardcoded Project IDs \n",
        "# and clean up complex incremental logic that might fail in a sandbox environment.\n",
        "def comprehensive_refactor(base_path):\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        # Skip internal dbt system directories (dbt_packages, logs, target) to avoid corrupting build artifacts.\n",
        "        if 'dbt_packages' in root or 'logs' in root or 'target' in root:\n",
        "            continue\n",
        "\n",
        "        for file in files:\n",
        "            if file.endswith(('.yml', '.sql', '.md', '.json')):\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "\n",
        "                if OLD_ID in content or \"materialized='incremental'\" in content or \"materialized: incremental\" in content:\n",
        "                    # A. Update Project ID: Replace the old hardcoded Project ID with the current user's Project ID.\n",
        "                    new_content = content.replace(OLD_ID, NEW_ID)\n",
        "\n",
        "                    # B. Materialization Strategy: Switch from 'incremental' to 'table' materialization \n",
        "                    # to simplify the workflow and avoid complexity in this sandbox environment.\n",
        "                    new_content = new_content.replace(\"materialized='incremental'\", \"materialized='table'\")\n",
        "                    new_content = new_content.replace(\"materialized: incremental\", \"materialized: table\")\n",
        "\n",
        "                    # C. Code Cleanup: Remove complex jinja blocks associated with incremental logic \n",
        "                    # (like is_incremental()) to ensure cleaner execution.\n",
        "                    new_content = re.sub(r\"{% if is_incremental\\(\\) %}.*?{% endif %}\", \"\", new_content, flags=re.DOTALL)\n",
        "\n",
        "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                        f.write(new_content)\n",
        "                    print(f\"âœ… Updated and refactored: {file}\")\n",
        "\n",
        "# 3. Execute Refactoring\n",
        "# Run the refactoring function across the entire dbt project directory.\n",
        "comprehensive_refactor(DBT_PATH)\n",
        "\n",
        "# 4. Fix Project Configuration (dbt_project.yml)\n",
        "# Specifically target `dbt_project.yml` (the heart of the project) to ensure global materialization settings \n",
        "# are correct and remove any lingering deprecated incremental configurations.\n",
        "project_conf_path = os.path.join(DBT_PATH, \"dbt_project.yml\")\n",
        "if os.path.exists(project_conf_path):\n",
        "    with open(project_conf_path, 'r') as f:\n",
        "        conf = f.read()\n",
        "    # Ensure explicit table materialization is set to avoid deprecation warnings.\n",
        "    conf = conf.replace(\"materialized: table\", \"materialized: table\")\n",
        "    # regex substitution to guarantee incremental logic is removed from models config.\n",
        "    conf = re.sub(r\"materialized: incremental\", \"materialized: table\", conf)\n",
        "    with open(project_conf_path, 'w') as f:\n",
        "        f.write(conf)\n",
        "    print(\"ğŸ¯ Fully repaired dbt_project.yml.\")\n",
        "\n",
        "# 5. Fix dim_orders Model\n",
        "# Overwrite the `dim_orders.sql` model with a standard BigQuery SQL definition \n",
        "# to fix casting and timestamp issues, ensuring it uses Standard SQL dialect.\n",
        "orders_model_path = os.path.join(DBT_PATH, \"models/marts/dim_orders.sql\")\n",
        "final_sql = \"\"\"\n",
        "{{ config(materialized='table') }}\n",
        "SELECT\n",
        "    order_id, customer_id, order_status,\n",
        "    SAFE_CAST(order_purchase_timestamp AS TIMESTAMP) AS order_purchase_timestamp,\n",
        "    SAFE_CAST(order_approved_at AS TIMESTAMP) AS order_approved_at,\n",
        "    SAFE_CAST(order_delivered_carrier_date AS TIMESTAMP) AS order_delivered_carrier_date,\n",
        "    SAFE_CAST(order_delivered_customer_date AS TIMESTAMP) AS order_delivered_customer_date,\n",
        "    SAFE_CAST(order_estimated_delivery_date AS TIMESTAMP) AS order_estimated_delivery_date,\n",
        "    TIMESTAMP_TRUNC(SAFE_CAST(order_purchase_timestamp AS TIMESTAMP), DAY) AS order_purchase_date\n",
        "FROM {{ ref('dim_orders_stg') }}\n",
        "\"\"\"\n",
        "with open(orders_model_path, 'w') as f:\n",
        "    f.write(final_sql)\n",
        "\n",
        "print(\"\\nâœ¨ dbt project is now clean, connected to your project, and ready for execution!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“˜ Data Engineering Architecture & Concepts Explained\n",
        "\n",
        "Before executing the pipeline, it is crucial to understand the architectural decisions and data engineering concepts powering this project. This Modern Data Stack uses an **ELT (Extract, Load, Transform)** pattern, leveraging the power of BigQuery for processing.\n",
        "\n",
        "### 1. The Architecture Layers (Medallion Architecture)\n",
        "\n",
        "We organizes data into three distinct processing layers to ensure quality and usability:\n",
        "\n",
        "*   **ğŸ¥‰ Raw Layer (Bronze):** \n",
        "    *   **Source:** Kaggle CSV files.\n",
        "    *   **Action:** Direct ingestion into BigQuery (`ecommerce_raw` dataset).\n",
        "    *   **Characteristics:** Data is \"as-is\", with potential duplicates or data type issues. We perform minimal changes here (just column name sanitization).\n",
        "\n",
        "*   **ğŸ¥ˆ Staging Layer (Silver):** (`models/stg/`)\n",
        "    *   **Purpose:** Cleaning and Standardization.\n",
        "    *   **Key Techniques Used:**\n",
        "        *   **Deduplication:** Using SQL window functions (e.g., `QUALIFY COUNT(...) OVER(...)`) to ensure uniqueness, as seen in `dim_customers_stg` to resolve zip code conflicts.\n",
        "        *   **Type Casting:** Converting strings to Dates/Timestamps using `SAFE_CAST` to prevent pipeline failures on bad data.\n",
        "        *   **Renaming:** Changing technical column names to business-friendly terms.\n",
        "\n",
        "*   **ğŸ¥‡ Marts Layer (Gold):** (`models/marts/`)\n",
        "    *   **Purpose:** Business Analytics & Reporting.\n",
        "    *   **Design Pattern:** **Star Schema**.\n",
        "    *   **Components:**\n",
        "        *   **Facts (`fact_`):** Operational metrics (e.g., `fact_order_items`). These tables contain foreign keys and numeric measures (Price, Freight Value).\n",
        "        *   **Dimensions (`dim_`):** Contextual data (e.g., `dim_customers`, `dim_products`). These answer the \"Who\", \"What\", and \"Where\" questions.\n",
        "\n",
        "### 2. Deep Dive: Dimensional Modeling (Star Schema)\n",
        "\n",
        "The core of this warehouse allows for efficient querying by BI tools (like Looker or Tableau).\n",
        "\n",
        "*   **Center:** `fact_order_items` (The transaction).\n",
        "    *   *Links to:* `dim_customers` (Who bought it?)\n",
        "    *   *Links to:* `dim_products` (What was bought?)\n",
        "    *   *Links to:* `dim_sellers` (Who sold it?)\n",
        "    *   *Links to:* `dim_orders` (When and how was it delivered?)\n",
        "\n",
        "### 3. dbt (Data Build Tool) Concepts\n",
        "\n",
        "*   **Materialization (`table` vs `incremental`):**\n",
        "    *   You might notice our refactoring script forced `materialized='table'`. \n",
        "    *   **Incremental:** Best for massive datasets (PB scale). It only processes *new* data since the last run. It requires complex logic (`is_incremental()`) and unique keys.\n",
        "    *   **Table:** Rebuilds the entire table from scratch every run. For this scale (Brazilian E-Commerce dataset), `table` is simpler, less error-prone, and faster to debug in a development environment.\n",
        "    \n",
        "*   **Ref (`{{ ref('...') }}`):** \n",
        "    *   This is dbt's magic. Instead of hardcoding table names like `project.dataset.table`, we use `ref()`. This builds a **Dependency Graph (DAG)**, ensuring `dim_customers_stg` runs *before* `dim_customers`.\n",
        "\n",
        "### 4. Data Quality Checks\n",
        "\n",
        "*   **Generic Tests:** dbt checks for `unique` and `not_null` constraints automatically defined in `schema.yml`.\n",
        "*   **Custom Logic:** Our SQL transformations handle edge cases, such as `TIMESTAMP_TRUNC` in `dim_orders` to standardize purchase dates for daily aggregations.\n",
        "\n",
        "---\n",
        "*Ready to Build: The next cell executes `dbt run`, which compiles these SQL models into executable BigQuery code and runs them in the correct dependency order.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQDq6OaBj4Mb",
        "outputId": "58f980f3-41e8-4c24-bae0-0099ed5e668d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/modern-data-warehouse-bq-dbt/GCP-COLAB-DBT/pipeline-ecommerce-bq-dbt/cloud_run_dbt/dbt\n",
            "\u001b[0m22:31:21  Running with dbt=1.11.2\n",
            "\u001b[0m22:31:29  [\u001b[33mWARNING\u001b[0m][MissingPlusPrefixDeprecation]: Deprecated functionality\n",
            "Missing '+' prefix on `materialized` found at\n",
            "`pipeline_ecommerce.stg.materialized` in file `/content/modern-data-warehouse-bq\n",
            "-dbt/GCP-COLAB-DBT/pipeline-ecommerce-bq-dbt/cloud_run_dbt/dbt/dbt_project.yml`.\n",
            "Hierarchical config values without a '+' prefix are deprecated in\n",
            "dbt_project.yml.\n",
            "\u001b[0m22:31:30  Registered adapter: bigquery=1.11.0\n",
            "\u001b[0m22:31:30  Unable to do partial parsing because saved manifest not found. Starting full parse.\n",
            "\u001b[0m22:31:32  Found 16 models, 13 data tests, 537 macros\n",
            "\u001b[0m22:31:32  \n",
            "\u001b[0m22:31:32  Concurrency: 3 threads (target='dev')\n",
            "\u001b[0m22:31:32  \n",
            "\u001b[0m22:31:35  1 of 16 START sql table model ecommerce_stg.dim_customers_stg .................. [RUN]\n",
            "\u001b[0m22:31:35  2 of 16 START sql table model ecommerce_stg.dim_orders_stg ..................... [RUN]\n",
            "\u001b[0m22:31:35  3 of 16 START sql table model ecommerce_stg.dim_products_stg ................... [RUN]\n",
            "\u001b[0m22:31:39  2 of 16 OK created sql table model ecommerce_stg.dim_orders_stg ................ [\u001b[32mCREATE TABLE (99.4k rows, 17.3 MiB processed)\u001b[0m in 4.02s]\n",
            "\u001b[0m22:31:39  4 of 16 START sql table model ecommerce_stg.dim_sellers_stg .................... [RUN]\n",
            "\u001b[0m22:31:39  3 of 16 OK created sql table model ecommerce_stg.dim_products_stg .............. [\u001b[32mCREATE TABLE (33.0k rows, 3.3 MiB processed)\u001b[0m in 4.09s]\n",
            "\u001b[0m22:31:39  5 of 16 START sql table model ecommerce_stg.fact_order_items_stg ............... [RUN]\n",
            "\u001b[0m22:31:40  1 of 16 OK created sql table model ecommerce_stg.dim_customers_stg ............. [\u001b[32mCREATE TABLE (99.4k rows, 32.2 MiB processed)\u001b[0m in 4.18s]\n",
            "\u001b[0m22:31:40  6 of 16 START sql table model ecommerce_stg.fact_order_payments_stg ............ [RUN]\n",
            "\u001b[0m22:31:42  6 of 16 OK created sql table model ecommerce_stg.fact_order_payments_stg ....... [\u001b[32mCREATE TABLE (103.9k rows, 6.9 MiB processed)\u001b[0m in 2.82s]\n",
            "\u001b[0m22:31:42  7 of 16 START sql table model ecommerce_marts.dim_orders ....................... [RUN]\n",
            "\u001b[0m22:31:43  4 of 16 OK created sql table model ecommerce_stg.dim_sellers_stg ............... [\u001b[32mCREATE TABLE (3.1k rows, 23.6 MiB processed)\u001b[0m in 3.36s]\n",
            "\u001b[0m22:31:43  8 of 16 START sql table model ecommerce_marts.dim_products ..................... [RUN]\n",
            "\u001b[0m22:31:45  5 of 16 OK created sql table model ecommerce_stg.fact_order_items_stg .......... [\u001b[32mCREATE TABLE (112.7k rows, 15.8 MiB processed)\u001b[0m in 5.68s]\n",
            "\u001b[0m22:31:45  9 of 16 START sql table model ecommerce_marts.dim_customers .................... [RUN]\n",
            "\u001b[0m22:31:46  7 of 16 OK created sql table model ecommerce_marts.dim_orders .................. [\u001b[32mCREATE TABLE (99.4k rows, 17.3 MiB processed)\u001b[0m in 3.27s]\n",
            "\u001b[0m22:31:46  10 of 16 START sql table model ecommerce_marts.fact_order_payments ............. [RUN]\n",
            "\u001b[0m22:31:47  8 of 16 OK created sql table model ecommerce_marts.dim_products ................ [\u001b[32mCREATE TABLE (33.0k rows, 3.6 MiB processed)\u001b[0m in 4.69s]\n",
            "\u001b[0m22:31:47  11 of 16 START sql table model ecommerce_marts.dim_sellers ..................... [RUN]\n",
            "\u001b[0m22:31:48  9 of 16 OK created sql table model ecommerce_marts.dim_customers ............... [\u001b[32mCREATE TABLE (99.4k rows, 8.8 MiB processed)\u001b[0m in 3.02s]\n",
            "\u001b[0m22:31:48  12 of 16 START sql table model ecommerce_marts.fact_order_items ................ [RUN]\n",
            "\u001b[0m22:31:49  10 of 16 OK created sql table model ecommerce_marts.fact_order_payments ........ [\u001b[32mCREATE TABLE (103.9k rows, 7.7 MiB processed)\u001b[0m in 3.39s]\n",
            "\u001b[0m22:31:50  11 of 16 OK created sql table model ecommerce_marts.dim_sellers ................ [\u001b[32mCREATE TABLE (3.1k rows, 175.7 KiB processed)\u001b[0m in 2.79s]\n",
            "\u001b[0m22:31:52  12 of 16 OK created sql table model ecommerce_marts.fact_order_items ........... [\u001b[32mCREATE TABLE (112.7k rows, 16.7 MiB processed)\u001b[0m in 4.08s]\n",
            "\u001b[0m22:31:52  13 of 16 START sql view model ecommerce_marts.vw_fact_customer_purchase_summary  [RUN]\n",
            "\u001b[0m22:31:52  14 of 16 START sql view model ecommerce_marts.vw_fact_product_performance ...... [RUN]\n",
            "\u001b[0m22:31:52  15 of 16 START sql view model ecommerce_marts.vw_fact_products_sales_agg_time_analysis  [RUN]\n",
            "\u001b[0m22:31:53  14 of 16 OK created sql view model ecommerce_marts.vw_fact_product_performance . [\u001b[32mCREATE VIEW (0 processed)\u001b[0m in 1.07s]\n",
            "\u001b[0m22:31:53  16 of 16 START sql view model ecommerce_marts.vw_fact_seller_performance ....... [RUN]\n",
            "\u001b[0m22:31:53  13 of 16 OK created sql view model ecommerce_marts.vw_fact_customer_purchase_summary  [\u001b[32mCREATE VIEW (0 processed)\u001b[0m in 1.17s]\n",
            "\u001b[0m22:31:53  15 of 16 OK created sql view model ecommerce_marts.vw_fact_products_sales_agg_time_analysis  [\u001b[32mCREATE VIEW (0 processed)\u001b[0m in 1.18s]\n",
            "\u001b[0m22:31:54  16 of 16 OK created sql view model ecommerce_marts.vw_fact_seller_performance .. [\u001b[32mCREATE VIEW (0 processed)\u001b[0m in 1.11s]\n",
            "\u001b[0m22:31:54  \n",
            "\u001b[0m22:31:54  Finished running 12 table models, 4 view models in 0 hours 0 minutes and 22.07 seconds (22.07s).\n",
            "\u001b[0m22:31:54  \n",
            "\u001b[0m22:31:54  \u001b[32mCompleted successfully\u001b[0m\n",
            "\u001b[0m22:31:54  \n",
            "\u001b[0m22:31:54  Done. PASS=16 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=16\n",
            "\u001b[0m22:31:54  [\u001b[33mWARNING\u001b[0m][DeprecationsSummary]: Deprecated functionality\n",
            "Summary of encountered deprecations:\n",
            "- MissingPlusPrefixDeprecation: 1 occurrence\n",
            "To see all deprecation instances instead of just the first occurrence of each,\n",
            "run command again with the `--show-all-deprecations` flag. You may also need to\n",
            "run with `--no-partial-parse` as some deprecations are only encountered during\n",
            "parsing.\n"
          ]
        }
      ],
      "source": [
        "# 6. Execute dbt Pipeline transformation\n",
        "# This is the final execution step where the transformations defined in the SQL models are applied.\n",
        "\n",
        "# 1. Change Working Directory\n",
        "# We strictly navigate to the `dbt` subdirectory. \n",
        "# Why? The `dbt run` command looks for `dbt_project.yml` in the current folder. \n",
        "# Running it from outside this directory would result in a \"project not found\" error.\n",
        "%cd /content/modern-data-warehouse-bq-dbt/GCP-COLAB-DBT/pipeline-ecommerce-bq-dbt/cloud_run_dbt/dbt\n",
        "\n",
        "# 2. Trigger Transformation (dbt run)\n",
        "# The `dbt run` command performs the following actions:\n",
        "#   1. Compiles your Jinja SQL files (from models/) into standard SQL.\n",
        "#   2. Constructs the Dependency Graph (DAG) to determine the correct order (Staging -> Marts).\n",
        "#   3. Executes the DDL/DML statements one by one in BigQuery.\n",
        "#\n",
        "# Expected Output:\n",
        "#   - distinct \"OK created table\" messages for each model (dim_customers, fact_orders, etc.).\n",
        "#   - If any model fails, the pipeline stops and provides an error log.\n",
        "!dbt run"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
